from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch
from typing import List
import os
import openai
# from langdetect import detect

app = FastAPI()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("í™˜ê²½ë³€ìˆ˜ 'OPENAI_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
openai.api_key = api_key

# ëª¨ë¸ ê²½ë¡œ
model_path = "C:/Users/USER/Desktop/MainProj/nllb_finetuned_model_test"

# ê²½ë¡œ í™•ì¸
if not os.path.exists(model_path):
    raise FileNotFoundError(f"ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
print(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")
print(f"ë””ë ‰í† ë¦¬ ë‚´ìš©: {os.listdir(model_path)}")

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
try:
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=False,
        local_files_only=True
    )
    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_path,
        device_map="auto",
        local_files_only=True
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
except Exception as e:
    raise RuntimeError(f"ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}") from e

# ì§€ì› ì–¸ì–´ ëª©ë¡ (NLLB ì½”ë“œ ê¸°ì¤€)
LANGUAGES = {
    "í•œêµ­ì–´": "kor_Hang",
    "ì˜ì–´": "eng_Latn",
    "í¬ë©”ë¥´ì–´": "khm_Khmr",
    "ë²„ë§ˆì–´": "mya_Mymr",
    "ì¸ë„ë„¤ì‹œì•„ì–´": "ind_Latn",
    "ë„¤íŒ”ì–´": "npi_Deva",
    "ë² íŠ¸ë‚¨ì–´": "vie_Latn"
}

# ë¡œì»¬ NLLB ëª¨ë¸ë¡œ ë²ˆì—­ ìˆ˜í–‰ í•¨ìˆ˜
def translate_nllb(text: str, source_lang_code: str, target_lang_codes: List[str]) -> dict:
    if not text.strip():
        raise ValueError("ì…ë ¥ ë¬¸ì¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    print(f"ğŸ”¹ NLLB ë²ˆì—­ ìš”ì²­: '{text}' ({source_lang_code})")

    # ì…ë ¥ ë¬¸ì¥ ì•ì— ì†ŒìŠ¤ ì–¸ì–´ ì½”ë“œ ì¶”ê°€
    formatted_input = f"<<{source_lang_code}>> {text}"

    # ì…ë ¥ ë¬¸ì¥ í† í°í™”
    inputs = tokenizer(
        formatted_input,
        return_tensors="pt",
        max_length=128,
        truncation=True,
        padding="longest"
    )
    inputs = {key: val.to(device) for key, val in inputs.items()}

    # ê° ëŒ€ìƒ ì–¸ì–´ì— ëŒ€í•´ ë²ˆì—­ ìˆ˜í–‰
    results = {}
    for target_lang_code in target_lang_codes:
        with torch.no_grad():
            output_tokens = model.generate(
                **inputs,
                max_length=128,
                forced_bos_token_id=tokenizer.lang_code_to_id[target_lang_code],
                repetition_penalty=2.0,
                num_beams=5
            )
        translated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True).strip().replace('"', '')
        results[target_lang_code] = translated_text
        print(f"âœ… NLLB ë²ˆì—­ ì™„ë£Œ ({target_lang_code}): {translated_text}")

    return results

# OpenAIë¡œ ë§¥ë½ì— ë§ê²Œ ì¬ë²ˆì—­í•˜ëŠ” í•¨ìˆ˜
def translate_openai(original_text: str, nllb_result: str, target_language: str) -> str:
    prompt = (
        f"The following sentence's original text and NLLB translation are provided. "
        f"Refine the translation naturally in {target_language} and respond only in {target_language}. "
        f"Do not use any other language.\n\n"
        f"Original: {original_text}\n"
        f"NLLB Translation: {nllb_result}\n"
        f"Target Language: {target_language}"
    )
    
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": f"You are a highly skilled translator refining translations. Always respond in {target_language} only."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500,
        temperature=0.7
    )
    
    refined_text = response.choices[0].message["content"].strip()
    print(f"âœ… OpenAI ì¬ë²ˆì—­ ì™„ë£Œ ({target_language}): {refined_text}")
    return refined_text

# ìš”ì²­ ë°ì´í„° ëª¨ë¸
class TranslationRequest(BaseModel):
    text: str
    source_language: str
    target_languages: List[str]

@app.post("/translate/")
async def translate_text(request: TranslationRequest):
    try:
        # ì†ŒìŠ¤ ì–¸ì–´ì™€ ëŒ€ìƒ ì–¸ì–´ê°€ ì§€ì›ë˜ëŠ” ì–¸ì–´ì¸ì§€ í™•ì¸
        if request.source_language not in LANGUAGES:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ ì–¸ì–´ì…ë‹ˆë‹¤: {request.source_language}"
            )
        
        invalid_languages = [lang for lang in request.target_languages if lang not in LANGUAGES]
        if invalid_languages:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëŒ€ìƒ ì–¸ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {', '.join(invalid_languages)}"
            )

        # NLLBë¡œ ë²ˆì—­ ìˆ˜í–‰
        nllb_results = translate_nllb(
            request.text,
            LANGUAGES[request.source_language],
            [LANGUAGES[lang] for lang in request.target_languages]
        )
        
        # OpenAIë¡œ ì¬ë²ˆì—­ ìˆ˜í–‰
        translated_texts = {}
        for lang in request.target_languages:
            nllb_result = nllb_results[LANGUAGES[lang]]
            refined_text = translate_openai(request.text, nllb_result, lang)
            translated_texts[lang] = refined_text
        
        return {"translated_texts": translated_texts}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/translate/")
async def translate_get(text: str, source_language: str, target_languages: str):
    try:
        languages = [lang.strip() for lang in target_languages.split(",")]

        # ì†ŒìŠ¤ ì–¸ì–´ì™€ ëŒ€ìƒ ì–¸ì–´ê°€ ì§€ì›ë˜ëŠ” ì–¸ì–´ì¸ì§€ í™•ì¸
        if source_language not in LANGUAGES:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ ì–¸ì–´ì…ë‹ˆë‹¤: {source_language}"
            )
        
        invalid_languages = [lang for lang in languages if lang not in LANGUAGES]
        if invalid_languages:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëŒ€ìƒ ì–¸ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {', '.join(invalid_languages)}"
            )

        # NLLBë¡œ ë²ˆì—­ ìˆ˜í–‰
        nllb_results = translate_nllb(
            text,
            LANGUAGES[source_language],
            [LANGUAGES[lang] for lang in languages]
        )
        
        # OpenAIë¡œ ì¬ë²ˆì—­ ìˆ˜í–‰
        translated_texts = {}
        for lang in languages:
            nllb_result = nllb_results[LANGUAGES[lang]]
            refined_text = translate_openai(text, nllb_result, lang)
            translated_texts[lang] = refined_text
        
        return {"translated_texts": translated_texts}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/")
async def root():
    return {"message": "Welcome to the NLLB + OpenAI multi-language translation API! Use /translate/ endpoint."}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, port=8000)